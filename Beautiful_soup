1 使用Beautiful soup而不是正则式读取网页信息
2 CSS 
代码：
from urllib.request import urlopen
import re
from bs4 import BeautifulSoup

# if has Chinese, apply decode()
html = urlopen(
     "https://morvanzhou.github.io/static/scraping/basic-structure.html"
).read().decode('utf-8')

soup = BeautifulSoup(html, features='lxml')
print(soup.h1)

all_href = soup.find_all('a')
all_href = [l['href'] for l in all_href]
print('\n', all_href)

3 baidu爬虫
from bs4 import BeautifulSoup
from urllib.request import urlopen
import re
import random


base_url = "https://baike.baidu.com"
his = ["/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711"]

url = base_url + his[-1]

html = urlopen(url).read().decode('utf-8')
soup = BeautifulSoup(html, features='lxml')
print(soup.find('h1').get_text(), '    url: ', his[-1])

for i in range(20):
    url = base_url + his[-1]

    html = urlopen(url).read().decode('utf-8')
    soup = BeautifulSoup(html, features='lxml')
    print(i, soup.find('h1').get_text(), '    url: ', his[-1])

    # find valid urls
    sub_urls = soup.find_all("a", {"target": "_blank", "href": re.compile("^/item/(%.{2})+$")})

    if len(sub_urls) != 0:
        his.append(random.sample(sub_urls, 1)[0]['href'])
    else:
        # no valid sub link found
        his.pop()
